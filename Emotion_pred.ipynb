{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e17b1d",
   "metadata": {},
   "source": [
    "## Emotion recognition on twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a578b",
   "metadata": {},
   "source": [
    "1. After browsing the same Kaggle competetion last year, I figured out that my desired score on public leaderboard will be around __55 - 58__. Since there are only small changes between Public and Private leaderboard rankings, my score on the public dataset will be a very reliable indicator for my model proformance.\n",
    "\n",
    "\n",
    "2. Following code represent the prediction for our test dataset, if you want to know how I train the model, please go to another file called `Gen_model.py`. Since our training dataset is very large (1455563 data), so I took 5% of training data as my validation data to evaluate the model proformance.\n",
    "\n",
    "\n",
    "3. `Gen_model.py` was run on my lab server with GeForce RTX 2080 Ti(11GB), it took approximately __6 hours__ to train an epoch.\n",
    "\n",
    "\n",
    "4. The best score I recieved on public leaderboard is __0.56555__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bbaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, random_split,  SequentialSampler\n",
    "from transformers import BertForSequenceClassification,RobertaForSequenceClassification, AdamW, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from dataset import TweetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd23bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314966ab",
   "metadata": {},
   "source": [
    "### Preprocess - data cleaning ( Noted that content of tweets are too noisy )\n",
    "\n",
    "- I reckon that too much preprocess will decimate the expression of our tweets. So I just did some simple action to eliminate unnecessary content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc25188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final['text'] = df_final['text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ').str.replace('<lh>', '').str.strip()\n",
    "def preprocess_tweets(df):\n",
    "    # convert to lower case\n",
    "    df['text'] = df.text.str.lower()\n",
    "    # remove links\n",
    "    df.text = df.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', 'http', x))\n",
    "    df.text = df.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "    df.text = df.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "    df.text = df.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "    # substitute 'RT @' with '@'\n",
    "    df.text = df.text.apply(lambda x: re.compile('rt @').sub('@', x).strip())\n",
    "    # Remove usernames. The usernames are any word that starts with @.\n",
    "    df.text = df.text.apply(lambda x: re.sub('\\@[a-zA-Z0-9]*', '@user', x))\n",
    "    # convert '#' to '' and '_' to ' ' and ':' to ''\n",
    "    df.text = df.text.apply(lambda x: x.replace(\"<lh>\", ''))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df5c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read our data\n",
    "data_path = './data/'\n",
    "data_id = pd.read_csv(data_path + 'data_identification.csv')\n",
    "emotions = pd.read_csv(data_path + 'emotion.csv')\n",
    "sample_sub = pd.read_csv(data_path + 'sampleSubmission.csv')\n",
    "tweets = pd.read_json(data_path + 'tweets_DM.json', lines=True)\n",
    "tweets_important = pd.DataFrame(tweets._source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1164daae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a final dataframe from all of the data sources\n",
    "tw_list = tweets_important['_source'].to_list()\n",
    "tmp_df = pd.DataFrame.from_records(tw_list)\n",
    "tmp_df_list = tmp_df['tweet'].to_list()\n",
    "final_tweet_df = pd.DataFrame.from_records(tmp_df_list)\n",
    "df_final = pd.merge(final_tweet_df, data_id, how='outer', on='tweet_id').merge(emotions, how='outer', on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4600cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                hashtags  tweet_id  \\\n",
       "0                             [Snapchat]  0x376b20   \n",
       "1          [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                           [bibleverse]  0x28b412   \n",
       "3                                     []  0x1cd5b0   \n",
       "4                                     []  0x2de201   \n",
       "...                                  ...       ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531                               []  0x29d0cb   \n",
       "1867532                               []  0x2a6a4f   \n",
       "1867533                               []  0x24faed   \n",
       "1867534                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text identification  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...          train   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...          train   \n",
       "2        Confident of your obedience, I write to you, k...           test   \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>          train   \n",
       "4        \"Trust is not the same as faith. A friend is s...           test   \n",
       "...                                                    ...            ...   \n",
       "1867530  When you buy the last 2 tickets remaining for ...           test   \n",
       "1867531  I swear all this hard work gone pay off one da...           test   \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...           test   \n",
       "1867533  Ah, corporate life, where you can date <LH> us...          train   \n",
       "1867534             Blessed to be living #Sundayvibes <LH>          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "2                 NaN  \n",
       "3                fear  \n",
       "4                 NaN  \n",
       "...               ...  \n",
       "1867530           NaN  \n",
       "1867531           NaN  \n",
       "1867532           NaN  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d19369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     people who post \"add me on #snapchat\" must be ...\n",
       "1     @user as we see, trump is dangerous to #freepr...\n",
       "2     confident of your obedience, i write to you, k...\n",
       "3                       now issa is stalking tasha 😂😂😂 \n",
       "4     \"trust is not the same as faith. a friend is s...\n",
       "5     @user @user thx for the best time tonight. wha...\n",
       "6              still waiting on those supplies liscus. \n",
       "7                             love knows no gender. 😢😭 \n",
       "8     @user @user more highlights are being shown th...\n",
       "9     when do you have enough ? when are you satisfi...\n",
       "10    the #ssm debate;  (a manufactured fantasy used...\n",
       "11    i love suffering 🙃🙃 i love when valium does no...\n",
       "12    can someone tell my why my feeds scroll back t...\n",
       "13    you know you research butterflies when predict...\n",
       "14    my brother didn't tell me he was going to horr...\n",
       "15    on a scale of kylie jenner-heidi klum, i consi...\n",
       "16    progress at house meyer, pre galv sub frame go...\n",
       "17        vomi post birthday celebrations! #late_post  \n",
       "18    one of my dreams come true a couple of days ag...\n",
       "19       thankful for another day of life 🙏🏾. #blessed✨\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocess on our data\n",
    "df_final  = preprocess_tweets(df_final)\n",
    "df_final.text[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480dcd19",
   "metadata": {},
   "source": [
    "### Demonstration of RoBERTa tokenizer\n",
    " #### Why do I choose __RoBERTa__ as our model?\n",
    "  - Firstly, RoBERTa is a much robust model training with more data as its name tells. Secondly, its tokenizer can deal with __emoji__, which I think is very important to infer the emotion of tweets. On the other hand, BERT cannot deal tokenize the emoji, and that's the reason I chose RoBERTa over BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db25364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c8cb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  i love suffering 🙃🙃 i love when valium does nothing to help 🙃🙃 i love when my doctors say that they've done all they can 🙃🙃 \n",
      "Using roberta tokenizer: \n",
      " <s>i love suffering 🙃🙃 i love when valium does nothing to help 🙃🙃 i love when my doctors say that they've done all they can 🙃🙃 </s>\n",
      "Using bert tokenizer: \n",
      " [CLS] i love suffering [UNK] i love when valium does nothing to help [UNK] i love when my doctors say that they've done all they can [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Roberta tokenizer can handle with emoji\n",
    "print('Original text: ', df_final.text[11])\n",
    "tok = tokenizer_roberta.encode(df_final.text[11])\n",
    "tok2 = tokenizer_bert.encode(df_final.text[11])\n",
    "print('Using roberta tokenizer: \\n', tokenizer_roberta.decode(tok))\n",
    "print('Using bert tokenizer: \\n', tokenizer_bert.decode(tok2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf930e",
   "metadata": {},
   "source": [
    "### Hash tag enhancement (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c0a37a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_final['hashtags'] = df_final.hashtags.apply(lambda t: ' '.join(t))\\ndf_final['text'] = df_final['text'] + ' ' + df_final['hashtags']\\ndf_final['text']\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_final['hashtags'] = df_final.hashtags.apply(lambda t: ' '.join(t))\n",
    "df_final['text'] = df_final['text'] + ' ' + df_final['hashtags']\n",
    "df_final['text']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abb723",
   "metadata": {},
   "source": [
    "### Train - Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2c3e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA shape: (1455563, 5)\n",
      "TEST DATA shape: (411972, 5)\n"
     ]
    }
   ],
   "source": [
    "# Separate training from testing data\n",
    "train_df = df_final[df_final['identification'] == 'train']\n",
    "test_df = df_final[df_final['identification'] == 'test']\n",
    "# size of training and testing data\n",
    "print('TRAIN DATA shape:', train_df.shape)\n",
    "print('TEST DATA shape:', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc5fbf",
   "metadata": {},
   "source": [
    "### Visualize our data\n",
    "\n",
    "I observed a long-tailed distribution, so maybe using __weighted average loss function__ will be a good idea. But after eperiment, it didn't yeild a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330669ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's have a look at the distribution of the training data with respect to our targets\n",
    "\n",
    "train_df.emotion.value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650f9b8",
   "metadata": {},
   "source": [
    "### I was wondering the max length of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bc86e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['len'] = df_final.text.apply(lambda t:len(t))\n",
    "df_final.len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7058ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85809</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2489ba</td>\n",
       "      <td>similar to &amp;lt;aung san suu kyi&amp;gt; the &amp;lt;do...</td>\n",
       "      <td>train</td>\n",
       "      <td>surprise</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166030</th>\n",
       "      <td>[Bajrangi, Sultan, Wanted, Dabangg, EkThaTiger...</td>\n",
       "      <td>0x2e7cbd</td>\n",
       "      <td>for me,  #bajrangi &amp;gt; #sultan &amp;gt; #wanted &amp;...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171258</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x28abe1</td>\n",
       "      <td>a sunday reflection:&amp;gt;\"#ancestralwisdomwithh...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240753</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2cf4b8</td>\n",
       "      <td>dealing with an unhandled error because the ap...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249676</th>\n",
       "      <td>[Celtics, kyrieirvingtrade, offseason]</td>\n",
       "      <td>0x32254f</td>\n",
       "      <td>#celtics #kyrieirvingtrade  i get from work 2 ...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285588</th>\n",
       "      <td>[Soul_Thought, Divine_Intervention, ASK]</td>\n",
       "      <td>0x3700c1</td>\n",
       "      <td>#soul_thought &amp;lt;&amp;gt; let us  &amp;lt;&amp;gt; night ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302246</th>\n",
       "      <td>[YOU, ASSHOLE, TO]</td>\n",
       "      <td>0x2fa333</td>\n",
       "      <td>@user@user###@user@user   \"despise &amp; fuckin ha...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328603</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x200fd4</td>\n",
       "      <td>&amp;gt;goes to gym.  &amp;gt;ahh, good workout.  &amp;gt;...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345801</th>\n",
       "      <td>[NOW, I, DEFINITLY, YEP, IT, HAPPEND]</td>\n",
       "      <td>0x34987b</td>\n",
       "      <td>@user #now #i\"m  #definitly @user @user...... ...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415549</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x30024e</td>\n",
       "      <td>a reflection:&amp;gt;\"#southeastasianethniccleansi...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471953</th>\n",
       "      <td>[Lucky15, Newbury, Mar, Newbury, Mar]</td>\n",
       "      <td>0x31ae69</td>\n",
       "      <td>#lucky15  1:50 #newbury - straight right @use...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669840</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2ef06e</td>\n",
       "      <td>@user @user \"#ifyoucantbeatthem:&amp;gt;how&amp;if,&amp;gt...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759387</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x33f3cd</td>\n",
       "      <td>@user respects!&amp;gt;\"#better!&amp;gt;#better!!&amp;gt;b...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847978</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2ec26c</td>\n",
       "      <td>when it's the biggest fight of the century and...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897697</th>\n",
       "      <td>[Thanku]</td>\n",
       "      <td>0x2b5cb1</td>\n",
       "      <td>what a batting ###@user@user@user kaur@user@us...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043993</th>\n",
       "      <td>[Soul_Thought, God, God, GOD, God]</td>\n",
       "      <td>0x36104a</td>\n",
       "      <td>#soul_thought we are #god's eyes. &amp;lt;&amp;gt; we ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090992</th>\n",
       "      <td>[LOVE]</td>\n",
       "      <td>0x2fb9e6</td>\n",
       "      <td>wuthering height`s!!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129454</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x30bae8</td>\n",
       "      <td>my phone is @user@user@user@user@user@user@use...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191594</th>\n",
       "      <td>[Nostalgic]</td>\n",
       "      <td>0x38ec41</td>\n",
       "      <td>sleepovers back in the day when y'all \"go to s...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200110</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x3032bc</td>\n",
       "      <td>staring @user a father &amp; toddler eatin 2gether...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315893</th>\n",
       "      <td>[AP, G20, un, SNL, CNN, thehill, StarWarsDay, ...</td>\n",
       "      <td>0x2b5f8b</td>\n",
       "      <td>@user_ #ap #g20 #un #snl #cnn #thehill #starwa...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455313</th>\n",
       "      <td>[Soul_Thought, Jesus, REBORN]</td>\n",
       "      <td>0x2c7193</td>\n",
       "      <td>#soul_thought want a job? &amp;lt;&amp;gt; come to #je...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485604</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x30a0f6</td>\n",
       "      <td>::::::#good:::  :::#better:: ::::#best:  @user...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494172</th>\n",
       "      <td>[ConfessYourUnpopularOpinion, MichealEmerson, ...</td>\n",
       "      <td>0x214f84</td>\n",
       "      <td>#confessyourunpopularopinion #michealemerson a...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544852</th>\n",
       "      <td>[Cctv, CLICK_TECHNOLOGY]</td>\n",
       "      <td>0x34984a</td>\n",
       "      <td>&amp;gt;house &amp;gt;family &amp;gt;business &amp;gt;masjid ...</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739891</th>\n",
       "      <td>[TakeTheKnee]</td>\n",
       "      <td>0x301041</td>\n",
       "      <td>#taketheknee &amp;gt;what about my dad&amp;gt;&amp;gt;died...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741563</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x37e9cd</td>\n",
       "      <td>when he gets clingy on purpose and doesn't let...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746899</th>\n",
       "      <td>[GMan, God, GMan, God, GMan, God]</td>\n",
       "      <td>0x343ee4</td>\n",
       "      <td>#gman: god, do i gotta love everyone? &amp;lt;&amp;gt;...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817767</th>\n",
       "      <td>[spread, every]</td>\n",
       "      <td>0x2e0e49</td>\n",
       "      <td>\" children's@user are the best@user creation o...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  hashtags  tweet_id  \\\n",
       "85809                                                   []  0x2489ba   \n",
       "166030   [Bajrangi, Sultan, Wanted, Dabangg, EkThaTiger...  0x2e7cbd   \n",
       "171258                                                  []  0x28abe1   \n",
       "240753                                                  []  0x2cf4b8   \n",
       "249676              [Celtics, kyrieirvingtrade, offseason]  0x32254f   \n",
       "285588            [Soul_Thought, Divine_Intervention, ASK]  0x3700c1   \n",
       "302246                                  [YOU, ASSHOLE, TO]  0x2fa333   \n",
       "328603                                                  []  0x200fd4   \n",
       "345801               [NOW, I, DEFINITLY, YEP, IT, HAPPEND]  0x34987b   \n",
       "415549                                                  []  0x30024e   \n",
       "471953               [Lucky15, Newbury, Mar, Newbury, Mar]  0x31ae69   \n",
       "669840                                                  []  0x2ef06e   \n",
       "759387                                                  []  0x33f3cd   \n",
       "847978                                                  []  0x2ec26c   \n",
       "897697                                            [Thanku]  0x2b5cb1   \n",
       "1043993                 [Soul_Thought, God, God, GOD, God]  0x36104a   \n",
       "1090992                                             [LOVE]  0x2fb9e6   \n",
       "1129454                                                 []  0x30bae8   \n",
       "1191594                                        [Nostalgic]  0x38ec41   \n",
       "1200110                                                 []  0x3032bc   \n",
       "1315893  [AP, G20, un, SNL, CNN, thehill, StarWarsDay, ...  0x2b5f8b   \n",
       "1455313                      [Soul_Thought, Jesus, REBORN]  0x2c7193   \n",
       "1485604                                                 []  0x30a0f6   \n",
       "1494172  [ConfessYourUnpopularOpinion, MichealEmerson, ...  0x214f84   \n",
       "1544852                           [Cctv, CLICK_TECHNOLOGY]  0x34984a   \n",
       "1739891                                      [TakeTheKnee]  0x301041   \n",
       "1741563                                                 []  0x37e9cd   \n",
       "1746899                  [GMan, God, GMan, God, GMan, God]  0x343ee4   \n",
       "1817767                                    [spread, every]  0x2e0e49   \n",
       "\n",
       "                                                      text identification  \\\n",
       "85809    similar to &lt;aung san suu kyi&gt; the &lt;do...          train   \n",
       "166030   for me,  #bajrangi &gt; #sultan &gt; #wanted &...          train   \n",
       "171258   a sunday reflection:&gt;\"#ancestralwisdomwithh...          train   \n",
       "240753   dealing with an unhandled error because the ap...          train   \n",
       "249676   #celtics #kyrieirvingtrade  i get from work 2 ...          train   \n",
       "285588   #soul_thought &lt;&gt; let us  &lt;&gt; night ...          train   \n",
       "302246   @user@user###@user@user   \"despise & fuckin ha...          train   \n",
       "328603   &gt;goes to gym.  &gt;ahh, good workout.  &gt;...          train   \n",
       "345801   @user #now #i\"m  #definitly @user @user...... ...          train   \n",
       "415549   a reflection:&gt;\"#southeastasianethniccleansi...          train   \n",
       "471953    #lucky15  1:50 #newbury - straight right @use...          train   \n",
       "669840   @user @user \"#ifyoucantbeatthem:&gt;how&if,&gt...          train   \n",
       "759387   @user respects!&gt;\"#better!&gt;#better!!&gt;b...          train   \n",
       "847978   when it's the biggest fight of the century and...           test   \n",
       "897697   what a batting ###@user@user@user kaur@user@us...          train   \n",
       "1043993  #soul_thought we are #god's eyes. &lt;&gt; we ...          train   \n",
       "1090992  wuthering height`s!!!!!!!!!!!!!!!!!!!!!!!!!!!!...          train   \n",
       "1129454  my phone is @user@user@user@user@user@user@use...          train   \n",
       "1191594  sleepovers back in the day when y'all \"go to s...          train   \n",
       "1200110  staring @user a father & toddler eatin 2gether...          train   \n",
       "1315893  @user_ #ap #g20 #un #snl #cnn #thehill #starwa...          train   \n",
       "1455313  #soul_thought want a job? &lt;&gt; come to #je...          train   \n",
       "1485604  ::::::#good:::  :::#better:: ::::#best:  @user...          train   \n",
       "1494172  #confessyourunpopularopinion #michealemerson a...           test   \n",
       "1544852   &gt;house &gt;family &gt;business &gt;masjid ...          train   \n",
       "1739891  #taketheknee &gt;what about my dad&gt;&gt;died...          train   \n",
       "1741563  when he gets clingy on purpose and doesn't let...           test   \n",
       "1746899  #gman: god, do i gotta love everyone? &lt;&gt;...          train   \n",
       "1817767  \" children's@user are the best@user creation o...          train   \n",
       "\n",
       "              emotion  len  \n",
       "85809        surprise  154  \n",
       "166030          trust  163  \n",
       "171258   anticipation  158  \n",
       "240753        disgust  152  \n",
       "249676        disgust  160  \n",
       "285588   anticipation  153  \n",
       "302246        disgust  153  \n",
       "328603          trust  163  \n",
       "345801          trust  192  \n",
       "415549        disgust  155  \n",
       "471953        disgust  152  \n",
       "669840          trust  151  \n",
       "759387            joy  164  \n",
       "847978            NaN  155  \n",
       "897697          trust  152  \n",
       "1043993  anticipation  160  \n",
       "1090992           joy  157  \n",
       "1129454  anticipation  162  \n",
       "1191594       sadness  152  \n",
       "1200110           joy  152  \n",
       "1315893       disgust  156  \n",
       "1455313  anticipation  151  \n",
       "1485604           joy  153  \n",
       "1494172           NaN  248  \n",
       "1544852         trust  151  \n",
       "1739891  anticipation  152  \n",
       "1741563           NaN  154  \n",
       "1746899  anticipation  170  \n",
       "1817767           joy  154  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final2 = df_final[df_final['len'] > 150]\n",
    "df_final2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ea77a",
   "metadata": {},
   "source": [
    "### LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e96d3431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer we pretrain\n",
    "\n",
    "output_dir = './model_save/robert_lite_ep4'\n",
    "model = RobertaForSequenceClassification.from_pretrained(output_dir, num_labels = 8, \n",
    "                                                            output_attentions = False, \n",
    "                                                            output_hidden_states = False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, use_fast=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940d2b8",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d53c9f",
   "metadata": {},
   "source": [
    "Document suggests that training epcoh should be around 2 - 4 to get a better proformance.\n",
    "\n",
    "1. Using **bert-base-uncased model** training with 4 epochs -> 0.47927\n",
    "\n",
    "2. Using **bert-base-uncased model** training with 4 epochs + hashtag concatenation -> 0.53676\n",
    "\n",
    "3. Using **reoberta-base** training wih 2 epochs -> 0.56246\n",
    "\n",
    "4. Using **reoberta-base** training wih 4 epochs -> 0.56555\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79882136",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "tweet_id = test_df['tweet_id'].values\n",
    "prediction_data = TweetDataset(test_df, 'test', tokenizer)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5dfe3",
   "metadata": {},
   "source": [
    "### Start predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f5f4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 411,972 test sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/25749 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/james/anaconda3/envs/py36/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|█████████████████████████████████████████████████| 25749/25749 [39:56<00:00, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(tweet_id)))\n",
    "\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , pred_ID = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader):\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask = b_input_mask, return_dict = False)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "  # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9190ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 ... 3 1 3]\n"
     ]
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "print(flat_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc4560b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411972\n",
      "411972\n"
     ]
    }
   ],
   "source": [
    "print(len(flat_predictions))\n",
    "print(len(tweet_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f381afd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anticipation',\n",
       " 1: 'joy',\n",
       " 2: 'disgust',\n",
       " 3: 'sadness',\n",
       " 4: 'trust',\n",
       " 5: 'fear',\n",
       " 6: 'surprise',\n",
       " 7: 'anger'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'anticipation': 0, 'joy': 1, 'disgust': 2, 'sadness': 3, 'trust': 4, 'fear': 5, 'surprise': 6, 'anger': 7}\n",
    "index_map = {v: k for k, v in label_map.items()}\n",
    "index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "135b746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2c7743</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2c1eed</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2826ea</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x356d9a</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20fd95</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1dff4a</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x243512</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x27aa7d</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x1e983c</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x1e55ec</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       emotion\n",
       "0  0x2c7743           joy\n",
       "1  0x2c1eed  anticipation\n",
       "2  0x2826ea       sadness\n",
       "3  0x356d9a           joy\n",
       "4  0x20fd95           joy\n",
       "5  0x1dff4a       sadness\n",
       "6  0x243512           joy\n",
       "7  0x27aa7d  anticipation\n",
       "8  0x1e983c           joy\n",
       "9  0x1e55ec           joy"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions = [index_map[k] for k in flat_predictions]\n",
    "ID_EMO = dict(zip(tweet_id, final_predictions))\n",
    "sample_sub['emotion'] = sample_sub.id.apply(lambda t: ID_EMO[t])\n",
    "sample_sub.to_csv('sub.csv', index=False)  \n",
    "sample_sub[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0af2aa60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ade2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3711jvsc74a57bd082f90a6d6086172460f4c9c1e1a219fdde808fd64cbe4fc0c473da247502f162"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
